#  贪吃蛇 DQN 强化学习项目

这是一个使用深度强化学习（Deep Q-Network，DQN）算法来训练 AI 玩贪吃蛇游戏的项目。AI 通过观察游戏状态并学习最优策略，逐渐提高游戏得分。

## 📖 项目概述

贪吃蛇是一个经典游戏，玩家控制一条蛇在地图上移动，吃到食物后会变长，碰到墙壁或自己的身体则游戏结束。本项目使用 DQN 算法训练 AI 来玩这个游戏，通过不断尝试和学习，AI 能够学会如何避开障碍物并尽可能多地吃到食物。

## 📂 项目结构

```
main.py       # 主程序入口，包含训练循环和测试逻辑
env.py        # 贪吃蛇游戏环境实现，包含游戏规则和渲染逻辑
Agent.py      # DQN 智能体实现，包含神经网络模型和强化学习算法
myfont.ttf    # 游戏字体文件
```

## 🚀 技术特点

- **深度强化学习**: 使用 Deep Q-Network (DQN) 算法训练 AI
- **经验回放**: 使用经验回放缓冲区提高训练效率
- **探索与利用**: 通过 epsilon-greedy 策略平衡探索和利用
- **目标网络**: 使用目标网络稳定训练过程
- **PyGame 可视化**: 使用 PyGame 库实现游戏可视化

## ⚙️ 环境要求

- Python 3.6+
- PyTorch
- PyGame
- NumPy
- PIL (Pillow)
- torchvision

## 📥 安装指南

1. 克隆项目到本地

```bash
git clone [项目仓库URL]
cd 贪吃蛇-DQN
```

2. 安装依赖

```bash
pip install torch numpy pygame pillow torchvision
```

## 🎮 使用方法

1. 运行训练程序

```bash
python main.py
```

2. 程序运行后将开始训练过程，每 100 个回合会显示一次游戏界面，展示 AI 当前的表现。
3. 训练共进行 2000 个回合，训练过程中会实时输出回合数、奖励值和当前最大奖励值。
4. 当 AI 表现超过历史最佳记录时，会显示"当前最大奖赏值更新！"的提示。

## 🧠 模型说明

模型的状态空间由 10 个维度组成，包括蛇头方向、食物相对位置、危险位置等信息；动作空间包括上、下、左、右共 4 个动作。神经网络模型采用两个隐藏层的全连接结构，每层含有 128 个神经元。奖励机制方面，AI 吃到食物获得正奖励，死亡则得到负奖励，在其他时刻则给予小幅度负奖励以鼓励蛇尽快找到食物。

## 🔧 自定义参数

在 `Agent.py` 中可以修改以下参数来调整训练过程：

- `gamma`: 折扣因子，控制未来奖励的重要性
- `epsilon`、`epsilon_min`、`epsilon_decay`: 控制探索率及其衰减
- `learning_rate`: 学习率
- `batch_size`: 训练批次大小
- `memory_capacity`: 经验回放缓冲区容量

在 `env.py` 中可以修改以下参数来调整游戏环境：

- `windows_width`、`windows_height`: 游戏窗口大小
- `cell_size`: 网格大小
- `snake_speed`: 游戏速度

